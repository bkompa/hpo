{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''In this notebook we will classify the prescence/abscence of HPO terms for 183,000 PubMed Abstracts using \n",
    "Covultional Neural Networks (CNN). We need the following files:\n",
    "\n",
    "articles_hpo.txt : a map of articles to the appropriate HPO terms \n",
    "pmid_abstract.txt : a map of pmids and abstracts \n",
    "\n",
    "NOTE: pmid_abstract.txt includes articles that do not have an HPO term, so we \n",
    "will use the list of articles in articles_hpo.txt \n",
    "'''\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "\n",
    "import numpy as np \n",
    "import re \n",
    "import os \n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "#create and set the session for Tensorflow \n",
    "##K.set_session(sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing files...\n",
      "Creating DataFrame...\n",
      "Working with 157473 articles and 1307 hpo terms\n"
     ]
    }
   ],
   "source": [
    "#Getting the data \n",
    "\n",
    "#if the text files are somewhere else, change this \n",
    "data_path = os.getcwd()+'/data/'\n",
    "\n",
    "articles_hpo_file = open(data_path+'articles_hpo.txt')\n",
    "pmid_abstracts_file = open(data_path+'pmid_abstract.txt')\n",
    "\n",
    "#make an array of pmids and set of HPO terms \n",
    "print('Parsing files...')\n",
    "articles_hpo = {}\n",
    "pmids = set() \n",
    "hpo_terms = set() \n",
    "for line in articles_hpo_file:\n",
    "    parse = line.rstrip().split('\\t')\n",
    "    parse[0] = parse[0].replace('\"','')\n",
    "    articles_hpo[int(parse[0])] = '\\t'.join(parse[1:])\n",
    "    pmids.add(int(parse[0]))\n",
    "    hpo_terms |= set(parse[1:])\n",
    "\n",
    "#make a dictionary of abstracts \n",
    "pmid_abstracts = {}\n",
    "for line in pmid_abstracts_file:\n",
    "    parse = line.rstrip().split('\\t')\n",
    "    pmid_abstracts[int(parse[0])] = pmid_abstracts.get(parse[0],'')+'\\t'.join(parse[1:])\n",
    "    \n",
    "print('Creating DataFrame...')\n",
    "#make a matrix of articles and hpo terms using Pandas DataFrame\n",
    "df = pd.DataFrame(index=list(pmids), columns = list(hpo_terms))\n",
    "df = df.fillna(0)\n",
    "#fill in our dataframe \n",
    "for pmid in pmids: \n",
    "    for hpo_term in articles_hpo[pmid].split('\\t'):\n",
    "        df.loc[pmid,hpo_term] = 1\n",
    "    \n",
    "#make sure we are only dealing with articles that have abstracts and HPO terms \n",
    "pmids = pmids.intersection(set(pmid_abstracts.keys()))\n",
    "print('Working with {} articles and {} hpo terms'.format(len(pmids),len(hpo_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Graph, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense,Reshape,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class cnn_2d_model(object):\n",
    "    '''\n",
    "    This is a 2D convolutional model over word vectors\n",
    "    params is a dictonary with the following keys:\n",
    "    - 'word_vecs': initial values for word_vectors\n",
    "    - 'max_length': maximum length of the input sentences\n",
    "    - 'num_classes': number of target classes\n",
    "    - 'filter_sizes': list of n-gram filter sizes to use\n",
    "    - 'num_maps': number of feature maps to use in the convolutional layers\n",
    "    - 'num_hidden': number of units in hidden layer\n",
    "    '''\n",
    "    def __init__(self,params):\n",
    "        print(\"Initializing CNN model...\")\n",
    "        self.vocab_size = self.word_vecs.shape[0]\n",
    "        self.wv_dim = self.word_vecs.shape[1]\n",
    "        self.max_length = np.int(params['max_length'])\n",
    "        self.num_classes = np.int(params['num_classes'])\n",
    "        self.filter_sizes = params['filter_sizes']\n",
    "        self.num_maps = np.int(params['num_maps'])\n",
    "        self.num_hidden = np.int(params['num_hidden'])\n",
    "        self.window_size = np.int(params['window_size'])\n",
    "\n",
    "    def build_model(self, embedding_layer):\n",
    "        ## We'll need to use the graph model here to get the flexibility we need ##\n",
    "        input_layer = Input(shape=(self.max_length,))\n",
    "        ## The embedding layer is 3D tensor with shape (n_samples,batch_length,n_dim)\n",
    "        embedded_sequences = embedding_layer(input_layer)\n",
    "        grams = []\n",
    "        for n_gram in self.filter_sizes:\n",
    "            grams.append(generate_gram(n_gram, embedded_sequences))\n",
    "        merged = merge(grams, mode='concat', axis=1)\n",
    "        dense = Dense(self.num_hidden, activation='relu')(merged)\n",
    "        dense = Dense(self.num_classes, activation='softmax')(dense)\n",
    "\n",
    "        model = Model(input=input_layer,output=dense)\n",
    "        return model \n",
    "\n",
    "    def generate_gram(n_gram, embedded_sequences):\n",
    "        gram = Convolution2D(nb_filter=self.num_maps, nb_row=n_gram, nb_col=self.window_size, activation='relu')(embedded_sequences)\n",
    "        gram = MaxPooling2D(pool_size=(self.max_length - n_gram + 1, 1))(gram)\n",
    "        gram = Flatten()(gram)\n",
    "        gram = Dropout(0.5)(gram)\n",
    "        return gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "\n",
    "print('Thresholding dataframe...')\n",
    "TRESHOLD = 100 \n",
    "df_threshold = df.loc[:,np.sum(df.values,axis=0)>THRESHOLD]\n",
    "\n",
    "print('Compiling abstracts...')\n",
    "texts = []\n",
    "labels = []\n",
    "for pmid in df.index:\n",
    "    texts.append(pmid_abstract[pmid])\n",
    "    labels.append(df.loc[pmid,:].values)\n",
    "    \n",
    "#Tokenize the words\n",
    "print('Tokenizing the abstracts...')\n",
    "max_words = 20000\n",
    "tokenizer = Tokenizer(max_words)\n",
    "tokenizer.fit_on_texts(texts) #get the order of words by frequency\n",
    "texts_as_seq = tokenizer.text_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} words in the corpus'.format(len(word_index)))\n",
    "\n",
    "max_words_seq = 1000\n",
    "data = pad_sequences(texts_as_seq,max_words_seq)\n",
    "\n",
    "#train/test split \n",
    "validation_split = .2 \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, wv_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, \n",
    "                            wv_dim, \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make the cnn_2d object here\n",
    "model = cnn_2d_model.build_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
